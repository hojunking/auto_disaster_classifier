{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9ce20a-d17d-4350-b04b-f3220a8e240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ae2503c-5ef3-4365-942b-61ecb5af9978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "disaster_dataset = ['0218f76dff06752d', '02eab4f9ad6c0334', '0693141ad6ca7df3', '0bdfcf59dd6b55ce',\n",
    "                     '0ff480920c9928c5', '101b499a52d7f46a', '112caf3e075e5394', '132eac17a04c4517',\n",
    "                    '1550f14052bfec04', '183c1d7389c3b9ea', '1b24087aa0f9867b', '1df5638d77ff56cb',\n",
    "                    '1e2a454911be0c5f', '2341349abfc26e90', '24f67cf7b161ea6b', '2b3651e8ba31ee65',\n",
    "                    '2c67ed27579a6ea8', '345744962c7f4265', '34d50632542ecbb7', '35242c3f7a36c7bb', \n",
    "                    '36aee059e225bdaf', '3c6bf73ad74f1cd0', '3d789ac66fb6f955', '3f297fa8108336de',\n",
    "                    '3f75d6405bd8a83f', '56234c17e1d60456', '5b612117e5778a4f', '5c7d9dc0a050e458',\n",
    "                    '5d9d3da7f32e3ed0', '5dc549862bd6b8be', '6210ca5e86ae3d4e', '626d848b9b97ac60',\n",
    "                    '62f96f9bddee4d68', '676ed076e09c9831', '705926a75728804d', '73ec259eab888d00',\n",
    "                    '7633472d1ca39fe2', '78d846a0565d3bcd', '7b9ec91e0a4e22a9', '7e8eb56044f4b6fd',\n",
    "                    '7eaf75eb4e77e2fb', '818a844fd465ac22', '83b32c162d09a4f8', '84948dc5e331047f',\n",
    "                    '85f047ee06e75976', '88f9239e83cdb66d', '8b8fc6e31886cf55', '8e81cdd4461f27c4',\n",
    "                    '9272694dbd156cd5', '9e6328c01768659b', 'a052c41f030af85b', 'a0cfb16260ece855',\n",
    "                    'a2a59c80cef68850', 'a2e91881336cf349', 'a7a0b76240849487', 'a8f397a0f65a323c',\n",
    "                    'ade8e68f664d382f', 'b06aa91a6083c7c2', 'b3fec6a2d8c7536a', 'b85303b3bdfed978',\n",
    "                    'b8bc1b2847a360f1', 'be4d426c54e723b4', 'be5fb6daa44a3de5', 'bf8d2df0e0e9c22d',\n",
    "                    'bfdd567c7a0f948b', 'c5b72236f87adafc', 'c745a2779725f898', 'c9503eb51eee2f65', \n",
    "                    'cf2221aa10e29a2c', 'e22d5041146ddcf1', 'ef7aeb8f945dbab2', 'f5d7ec658b470898',\n",
    "                    'fd00a73e3f447913','022b54e4e857ed33', '0de39b3791f2baa6', '15657f348712e734', '25099167e4022eba',\n",
    "                    '26518efac4887b3c', '3797b764d267b064', '428509df148d1a27', '49aec59730a5a8aa',\n",
    "                    '51255a7a1548080e', '52fde99971a5708c', '58d12e852edf194f', '5bbe14ccb14cd541', \n",
    "                    '64285be9eb0c2a46', '6c7b295c1bceb158', '6e16070bbbf48bde', '721af8090dfd27aa',\n",
    "                    '874550cc18780297', '8a224c3b493fe762', '8cd19147d2c4f6b6', '91328c6645715517',\n",
    "                    'a545a8059df4801f', 'b80089edc6bbc0ed', 'd0d6ed4f1ccd7041', 'd1db43f8dc0c3780', \n",
    "                    'e05bc81aa1d691d8', 'efd4e8e3e46acafc', 'f3c2b2265ca7bc45', 'fc365a2dd199e835',\n",
    "                                        '00d425f0986ac579', '0128105921974df0', '01c4f65ea29a5632', '05b3130e553c6084',\n",
    "                    '153e26395bea1973', '1656a46450a881f5', '17dba0da31436843', '189f6a62d7fa224c',\n",
    "                    '2debae83fc6544cb', '4b2120ef0487e439', '502e1d8cb6b58911', '514efb5aee482d6f', \n",
    "                    '560e387f1b361a5c', '6a335f6ff3ffb6fd', 'bd62b456d26d69ea', 'd49698a15222f7fc',\n",
    "                    'db1890e3518f1218', 'dc68faebac1558fa', 'dd4ceaf81843211a', 'e674841ecccb5a3c',\n",
    "                    'fe2badae70e3f274', '002a6cf0d401929c', '009c5fb34590f1ca', '00c5ae08ce541b77','02272e5568435fda',\n",
    "                    '043ac011ff8afcf5', '04f67c62ee67f74b', '0589d82e0437d121', '0604cb9341631e15',\n",
    "                    '08fef6f3a0e3bbcb', '2c57d43b16d35db1', '3bba5ee24ec0ee54', '413a993b9bb464ab', \n",
    "                    '522c4fb0babb19f1', '55baf0e2c4186a11', '5763316a89f6298d', '6122e9cf06bd61ce',\n",
    "                    '6414f33c90e89519', '65258f8026ee8751', '678db2a731a13646', '6f7fc1281ee41e62',\n",
    "                    '7658e9d7ea54e4f6', '82e3aea3423ca41a', '96d9477747f15afd', 'a0f87eeae39baf57',\n",
    "                    'b74e7055559d0264', 'b8422e374555ab8a', 'b853a26933571d66','c478c70805c110f6',\n",
    "                    'da3ce31355e6391f','d6b1f98d28bf6e8a', 'd759ef76b9095152', 'dd45f271c163fe8c',\n",
    "                    'e7cef9a8ab2593a2', '02fd19498de1e94a', '290d59dab457a369', '295b7671477f50f9', '2e8300d0999c77b8', \n",
    "                    '31e0a1de0ee988a8', '3f63a70a78520377', '4080d016eb273aae', '7e2449f75a0a0acc', \n",
    "                    '9c24a74e0769736c' \n",
    "                   ]\n",
    "print(len(disaster_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce110136-8aaa-4034-a08c-8fd1a93b617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image1 = cv2.imread(\"../Data/open_image_disaster/002a6cf0d401929c.jpg\")\n",
    "#image2 = cv2.imread(\"../Data/open_image_disaster/b06aa91a6083c7c2.jpg\")\n",
    "#real_image = []\n",
    "\n",
    "\n",
    "# Declare an augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.Resize(height = 400, width = 400),\n",
    "    A.RandomBrightnessContrast(always_apply=False, p=1.0, brightness_limit=(0.00, 0.00), contrast_limit=(-0.2, 0.2), brightness_by_max=False),\n",
    "    A.SafeRotate(always_apply=True, p=0.5, limit=(-20, 20), interpolation=2, border_mode=0, value=(0, 0, 0), mask_value=None),\n",
    "    A.HorizontalFlip(always_apply=False, p=0.5),\n",
    "    A.CenterCrop(always_apply=True, p=1.0, height=256, width=256)\n",
    "    #    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0)\n",
    "    #A.pytorch.transforms.ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de84bb9-a204-42e2-be97-32b5daf9404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = []\n",
    "real_image = []\n",
    "\n",
    "for i in disaster_dataset:\n",
    "    im_bgr = cv2.imread(\"../Data/open_image_disaster/\" + i +\".jpg\")\n",
    "    img = im_bgr[:, :, ::-1]\n",
    "    real_image.append(img)\n",
    "    augmentation.append(transform(image=img)[\"image\"])\n",
    "\n",
    "augmentation = np.array(augmentation)\n",
    "print(len(augmentation))\n",
    "\n",
    "plt.figure(figsize=(10,200))\n",
    "\n",
    "s = 60\n",
    "e = 70\n",
    "c = 0\n",
    "for i in range(s, e):\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.subplot(40,2,c*2+1)\n",
    "    plt.imshow(real_image[i], cmap=plt.cm.binary)\n",
    "    plt.subplot(40,2,c*2+2)\n",
    "    plt.imshow(augmentation[i], cmap=plt.cm.binary)\n",
    "    c+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a090ae-0c08-4b29-8add-d3053221bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = '3f63a70a78520377'\n",
    "\n",
    "plt.figure(figsize=(10,200))\n",
    "for i in range(8):\n",
    "    im_bgr = cv2.imread(\"../Data/open_image_disaster/\" + test_img +\".jpg\")\n",
    "    img = im_bgr[:, :, ::-1]\n",
    "    \n",
    "    plt.grid(False)\n",
    "    plt.subplot(40,2,i+1)\n",
    "    plt.imshow(transform(image=img)[\"image\"], cmap=plt.cm.binary)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa7bc1-4713-42df-874c-9b000e33e925",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478716df-c5f7-451d-9f73-2fd7a321b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@555.131] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('../Data/open_image_disaster/3f63a70a78520377.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m test_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3f63a70a78520377\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m im_bgr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/open_image_disaster/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m test_img \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mim_bgr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m trans_img \u001b[38;5;241m=\u001b[39m transforms(image\u001b[38;5;241m=\u001b[39mimg)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m trans_img\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sample_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_img\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_01.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "save_path = '../Data/augmented/'\n",
    "test_img = '3f63a70a78520377'\n",
    "im_bgr = cv2.imread(\"../Data/open_image_disaster/\" + test_img +\".jpg\")\n",
    "img = im_bgr[:, :, ::-1]\n",
    "trans_img = transforms(image=img)['image']\n",
    "\n",
    "trans_img.save(os.path.join(save_path, f\"{test_img}_01.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c32aa1-1d7c-4c4c-8d5c-8d865824dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentation(image, nb_of_augmentation):\n",
    "    images = []\n",
    "    i = 0\n",
    "    for i in range(nb_of_augmentation):\n",
    "        x_batch = transform(image=image)['image']\n",
    "        images.append(x_batch['image'])\n",
    "        i += 1\n",
    "        if i >= nb_of_augmentation:\n",
    "            break\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435175a-25c5-428d-a510-f1ec0412a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(images, use_augmentation=False, nb_of_augmentation=1):\n",
    "    X = []\n",
    "    y = []\n",
    "    for x_ in images:\n",
    "        \n",
    "        # scaling pixels between 0.0-1.0\n",
    "        x_ = x_ / 255.\n",
    "        \n",
    "        # data Augmentation\n",
    "        if use_augmentation:\n",
    "            argu_img = image_augmentation(x_, nb_of_augmentation)\n",
    "            for a in argu_img:\n",
    "                X.append(a.reshape(28, 28, 1))\n",
    "                y.append(y_)\n",
    "\n",
    "        X.append(x_)\n",
    "        y.append(y_)\n",
    "    print('*Preprocessing completed: %i samples\\n' % len(X))\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d880ec-1fd7-4223-9f90-ed2c55d171d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = preprocess_data(\n",
    "    X_train,\n",
    "    use_augmentation=True, \n",
    "    nb_of_augmentation=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bdfd2-fd7a-4b34-be9c-edf260610569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
